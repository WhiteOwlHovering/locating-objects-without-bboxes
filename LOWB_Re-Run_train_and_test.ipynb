{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the model is feed the correct GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train model to test if code can duplicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] --train-dir TRAIN_DIR [--val-dir VAL_DIR] [--imgsize HxW]\r\n",
      "                [--batch-size N] [--epochs N] [--nThreads N] [--lr LR] [-p P]\r\n",
      "                [--no-cuda] [--no-data-augm] [--drop-last-batch] [--seed S]\r\n",
      "                [--resume PATH] [--save PATH] [--log-interval N]\r\n",
      "                [--max-trainset-size N] [--max-valset-size N] [--val-freq F]\r\n",
      "                [--visdom-env NAME] [--visdom-server SRV] [--visdom-port PRT]\r\n",
      "                [--optimizer OPTIM] [--replace-optimizer] [--max-mask-pts M]\r\n",
      "                [--paint] [--radius R] [--n-points N] [--ultrasmallnet]\r\n",
      "                [--lambdaa L]\r\n",
      "\r\n",
      "BoundingBox-less Location with PyTorch.\r\n",
      "\r\n",
      "MANDATORY arguments:\r\n",
      "  --train-dir TRAIN_DIR\r\n",
      "                        Directory with training images. Must contain image\r\n",
      "                        files (any format), and a CSV or XML file containing\r\n",
      "                        groundtruth, as described in the README.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --val-dir VAL_DIR     Directory with validation images and GT. If 'auto',\r\n",
      "                        20% of the training samples will be removed from\r\n",
      "                        training and used for validation. If left blank, no\r\n",
      "                        validation will be done.\r\n",
      "  --imgsize HxW         Size of the input images (height x width). (default:\r\n",
      "                        256x256)\r\n",
      "  --batch-size N        Input batch size for training. (default: 1)\r\n",
      "  --epochs N            Number of epochs to train. (default: inf)\r\n",
      "  --nThreads N, -j N    Number of threads to create for data loading. Must be\r\n",
      "                        a striclty positive int (default: 4)\r\n",
      "  --lr LR               Learning rate (step size). (default: 4e-05)\r\n",
      "  -p P                  alpha in the generalized mean (-inf => minimum)\r\n",
      "                        (default: -1)\r\n",
      "  --no-cuda             Disables CUDA training (default: False)\r\n",
      "  --no-data-augm        Disables data augmentation (random vert+horiz flip)\r\n",
      "                        (default: False)\r\n",
      "  --drop-last-batch     Drop the last batch during training, which may be\r\n",
      "                        incomplete. If the dataset size is not divisible by\r\n",
      "                        the batch size, then the last batch will be smaller.\r\n",
      "                        (default: False)\r\n",
      "  --seed S              Random seed. (default: 1)\r\n",
      "  --resume PATH         Path to latest checkpoint.\r\n",
      "  --save PATH           Where to save the model after each epoch.\r\n",
      "  --log-interval N      Time to wait between every time the losses are printed\r\n",
      "                        (in seconds). (default: 3)\r\n",
      "  --max-trainset-size N\r\n",
      "                        Only use the first N images of the training dataset.\r\n",
      "                        (default: inf)\r\n",
      "  --max-valset-size N   Only use the first N images of the validation dataset.\r\n",
      "                        (default: inf)\r\n",
      "  --val-freq F          Run validation every F epochs. If 0, no validation\r\n",
      "                        will be done. If no validation is done, a checkpoint\r\n",
      "                        will be saved every F epochs. (default: 1)\r\n",
      "  --visdom-env NAME     Name of the environment in Visdom. (default:\r\n",
      "                        default_environment)\r\n",
      "  --visdom-server SRV   Hostname of the Visdom server. If not provided,\r\n",
      "                        nothing will be sent to Visdom.\r\n",
      "  --visdom-port PRT     Port of the Visdom server. (default: 8989)\r\n",
      "  --optimizer OPTIM, --optim OPTIM\r\n",
      "                        SGD or Adam. (default: sgd)\r\n",
      "  --replace-optimizer   Replace optimizer state when resuming from checkpoint.\r\n",
      "                        If True, the optimizer will be replaced using the\r\n",
      "                        arguments of this scripts. If not resuming, it has no\r\n",
      "                        effect. (default: False)\r\n",
      "  --max-mask-pts M      Subsample this number of points from the mask, so that\r\n",
      "                        GMM fitting runs faster. (default: inf)\r\n",
      "  --paint               Paint red circles at the estimated locations in\r\n",
      "                        validation. This maskes it run much slower! (default:\r\n",
      "                        False)\r\n",
      "  --radius R            Detections at dist <= R to a GT pointare considered\r\n",
      "                        True Positives. (default: 5)\r\n",
      "  --n-points N          If you know the number of points (e.g, just one\r\n",
      "                        pupil), then set it. Otherwise it will be estimated.\r\n",
      "  --ultrasmallnet       If True, the 5 central layers are removed,resulting in\r\n",
      "                        a much smaller UNet. This is used for example for the\r\n",
      "                        pupil dataset.Make sure to enable this if your are\r\n",
      "                        restoring a checkpoint that was trained using this\r\n",
      "                        option enabled. (default: False)\r\n",
      "  --lambdaa L           Weight that will increase the importance of estimating\r\n",
      "                        the right number of points. (default: 1)\r\n"
     ]
    }
   ],
   "source": [
    "# Old batch size of 32, not fitting due to GPU size limit\n",
    "!python -m object-locator.train -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m object-locator.train \\ \n",
    "#     --train-dir '../../mall_dataset/frames' \\\n",
    "#     --batch-size 25 \\\n",
    "#     --lr 4e-05 \\\n",
    "#     --lambdaa 1 \\\n",
    "#     --visdom-env 'main' \\\n",
    "#     --visdom-server 'http://localhost' \\\n",
    "#     --val-dir '../../mall_dataset/frames' \\\n",
    "#     --optim Adam \\\n",
    "#     --max-valset-size 500 \\\n",
    "#     --save 'saved_model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.remove('../saved_model.ckpt')\n",
    "# epoch 828 at 0.1 LR, then down tuned to 0.001 (BS moved from 5 to 25)\n",
    "!python -m object-locator.train \\\n",
    "    --resume '../saved_model.ckpt' \\\n",
    "    --train-dir '../mall_dataset/frames' \\\n",
    "    --batch-size 25 \\\n",
    "    --imgsize '256x256' \\\n",
    "    --lr 0.001 \\\n",
    "    --lambdaa 1 \\\n",
    "    --visdom-env 'main' \\\n",
    "    --visdom-server 'http://localhost' \\\n",
    "    --val-dir '../mall_dataset/frames' \\\n",
    "    --val-freq 3 \\\n",
    "    --optim sgd \\\n",
    "    --max-valset-size 100 \\\n",
    "    --save '../saved_model.ckpt' \\\n",
    "#     --paint \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the following to their model loaded from a saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m object-locator.locate \\\n",
    "#        --dataset '../../mall_dataset/frames' \\\n",
    "#        --out './output_papers_model/' \\\n",
    "#        --model './checkpoints/mall,lambdaa=1,BS=32,Adam,LR1e-4.ckpt' \\\n",
    "#        --evaluate \\\n",
    "#        --no-gpu \\\n",
    "#        --radii 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate re-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m object-locator.locate \\\n",
    "#        --dataset '../../mall_dataset/frames' \\\n",
    "#        --out './output_retrained_model/' \\\n",
    "#        --model 'saved_model.ckpt' \\\n",
    "#        --evaluate \\\n",
    "#        --no-gpu \\\n",
    "#        --radii 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 640 by 480 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "object-locator",
   "language": "python",
   "name": "object-locator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
